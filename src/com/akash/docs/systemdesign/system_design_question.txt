rate limitor
tinyUrl
facebook feed page / twitter feed page
dropbox or google drive


book my show
Uber
make my trip
circuit breaker

==========

=>Rate limitor

/api1
/api2
/api3
/api4

client1
client2

config map (key = client + api)
{
	client = client1
	api = api1
	window = 10 sec
	threshold = 1000
}
{
	client = client1
	api = api1
	window = 60 sec
	threshold = 5000
}
{
	client = client2
	api = api2
	window = 60 sec
	threshold = 5000
}

http status = TOO_MANY_REQUESTS

data store format
key = client + api + window
value = order list of timestamp


client1 hits api1 (RateLimiterFilter)
(A) get window and threshold info by client + api combination
(B) prepare list of keys for all windows falling in client + api combination
for eg: keys =  client1_api1_10 , client1_api1_60

(C) check from data store if any of these keys are have exceeded their threshold
(D) if yes then return TOO_MANY_REQUESTS , else proceed

distributed solution : redis cluster
data structure : sorted set in redis

-> Step C
redis key = client + api + window
value (sorted set) = time_in_ms : time_in_ms

rateLimiterThresholdExceed {

	allow = true

	ZREMRANGEBYSCORE  key  '-inf' (currentTime - window)

	count = ZCOUNT  key  '-inf'  '+inf'
	if(count > threshold) {
		allow = false
	}

	ZADD  key  currentTime  currentTime

	return allow
}

-> Algo analysis :

ZREMRANGEBYSCORE
O(log(N)+M)

ZCOUNT
O(log(N))

ZADD
O(log(N))

(Q) what should be ttl for cache key
-> ?


==================

tinyUrl

short url -> long url
long url -> short url
Track click stats
Delete expired URLs


DB table
id, long_url, short_url, created_at, created_by

apis :
/getTinyUrl
/getLongUrl

Data is stored in DB and in a distributed cache

=> getTinyUrl
(1) check in distributed cache, if found then return
(2) check in DB , if found then return
(3) create tiny url for long url
(4) save in DB
(5) save in distributed cache


=> getLongUrl
(1) check in distributed cache, if found then return
(2) check in DB , if found then return


(Q) How to convert long string to short unique string
-> first we need to generate unique hash via MD5 or SHA256
-> then perform Base26 or Base64 encoding on that hash


(Q) what should be ttl for cache key
-> 1 day

(Q) how to delete expired urls
-> cron job should run after 24 hours and delete urls older than 5 days

(Q) how to track click stats
-> click stats should be maintained in distributed cache and
   should be updated in DB in separate table via cron
   cron time for stats update should be lesser (preferably half time) than delete cron


======================

Design facebook feed/Twitter page

facebook feed page - post
user -> friends
user -> post text/images
user -> can see post of his friends
post -> like, share, comment
//post will be visible on priority basis, based on time, num_of_comments and num_of_likes



User table
id, name, password, status

Friend table
id, user_id_1, user_id_2, status

Post table
id, post, posted_by, posted_at

Comment table
id, comment, post_id, commented_by, commented_at


apis
/createPost
/feedpage/{user_id}
/getFriends/{user_id}
/getComments/{post_id}


=> Approach 1

-> createPost
db insert post


-> feedpage , show paginated data (page_size=10, page_number=1)
get all friends (store friends in distributed cache, ttl = ? )
get top 10 latest post of each friend from DB
merge whole list into final 10 posts and return


=> Approach 2

Creation of new facebook post will save data in mysql
and push data in a pipeline (debizium --> kafka --> storm)
and every user related feeds are stored in a distributed cache like redis

Reads will be fulfilled from redis cache itself
No need to fire query on DB in case of Reads

kafka -->  Feed service --> Timeline service

-> Redis cache structure :
Every post or comment will be pushed to pipeline
Feed service will find all friends of the author of post
This post is pushed to all buckets (sorted set) of users in redis

sorted set logic by { time, num_of_likes, num_of_comments }
user1 --> [post_id_1, post_id_2]
user2 --> [post_id_1, post_id_2, post_id_3]


In this approach we have pre-populated all data of user feed in redis.
If any part of system is down, it will seen Eventually.
So it is Eventually consistent system.

(Q) If pipeline is down for a short time, how will it ensure that every packet is processed.
-> Using kafka with Storm will ensure at least once delivery
https://medium.com/@madhur25/meaning-of-at-least-once-at-most-once-and-exactly-once-delivery-10e477fafe16


(Q) If a user has 100k friends, then on his single post system will trigger 100k packets in redis.
This will be a bottleneck for redis, high load on timeline service.
-> For this special case, we can apply another logic of fetching data from DB (mixture of both DB and redis)
-> Or we can create separate redis keys for these special users, for these cases redis will store single copy of post.

=======================

Design dropbox or google drive

1) files can be uploaded or downloaded by users
2) files can be shared accross multiple devices and between multiple users
3) maintain history of changes in files


=> Approach 1

upload files to amazon S3, for scale, availabity and CDN properties
upload whole file to S3 when created
On any changes in file, upload full file to different path on S3 with different version
All these file meta informations can be stored in DB.

problems with this approach :
It consumes more internet bandwidth as on every update we upload full file again
It consumes more S3 space on higher updates (S3 space costs to us)

=> Approach 2

Divide the file into chunks of fixed size and upload these chunks in S3.
This will reduce latency and reduce S3 space on higher number of updates.
Chunk file uploaded to S3 will be having a unique random name based on some hash value.
These hash names, chunk number, file name, file type, file size
and all other similar information is stored in file_metadata.
This metadata is stored in DB.


Now how to upload file chunks to server.
To solve this problem we will use client side code (mobile client or web browser app)

Every client will be having a watcher service that watches on File changes.
Once a file is changed, the chunk file that is changed is uploaded to S3
and its metadata is created and updated to server via some api.
This metadata is saved to DB and cache.
Also this metadata is pushed to a queue like kafka , where it sends updated metadata packet to all other clients.
This way all clients sharing that file will be notified.
Advantage of queue is that, if client is offline for some time and it came again
then kafka can hold buffer data for a while.

mysql can be used to store chunk file metadata.

============================
