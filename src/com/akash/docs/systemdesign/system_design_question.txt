
*** https://hackernoon.com/top-10-system-design-interview-questions-for-software-engineers-8561290f0444
https://www.educative.io/courses/grokking-the-system-design-interview
https://www.educative.io/courses/grokking-the-object-oriented-design-interview
https://www.educative.io/courses/coderust-hacking-the-coding-interview


- Rate limiter
- tiny url system
- Facebook / twitter feed page ( async streaming pipeline)
- dropbox or google drive
- Leader board (redis)
- Big file processing system
- coupon code checkout system
- ticket booking system ( movie, railway)
- video streaming system (hotstar, netflix)
- shopping website system (amazon, myntra)
- online cab system (Uber, Ola)
- online travelling system (MMT)
- audit user actions and apply recommendation

- circuit breaker - hystrix
- api gateway -- zuul
- varnish
- Web sockets
    https://youtu.be/i5OVcTdt_OU
    Always opened tcp connection with bidirectional communication


SAGA : https://www.infoq.com/articles/saga-orchestration-outbox/

==========

=>Rate limitor

/api1
/api2
/api3
/api4

client1
client2

config map (key = client + api)
{
	client = client1
	api = api1
	window = 10 sec
	threshold = 1000
}
{
	client = client1
	api = api1
	window = 60 sec
	threshold = 5000
}
{
	client = client2
	api = api2
	window = 60 sec
	threshold = 5000
}

http status = TOO_MANY_REQUESTS

data store format
key = client + api + window
value = order list of timestamp


client1 hits api1 (RateLimiterFilter)
(A) get window and threshold info by client + api combination
(B) prepare list of keys for all windows falling in client + api combination
for eg: keys =  client1_api1_10 , client1_api1_60

(C) check from data store if any of these keys are have exceeded their threshold
(D) if yes then return TOO_MANY_REQUESTS , else proceed

distributed solution : redis cluster
data structure : sorted set in redis

-> Step C
redis key = client + api + window
value (sorted set) = time_in_ms : time_in_ms

rateLimiterThresholdExceed {

	allow = true

	ZREMRANGEBYSCORE  key  '-inf' (currentTime - window)

	count = ZCOUNT  key  '-inf'  '+inf'
	if(count > threshold) {
		allow = false
	}

	ZADD  key  currentTime  currentTime

	return allow
}

-> Algo analysis :

ZREMRANGEBYSCORE
O(log(N)+M)

ZCOUNT
O(log(N))

ZADD
O(log(N))

(Q) what should be ttl for cache key
-> ?


==================

tinyUrl

short url -> long url
long url -> short url
Track click stats
Delete expired URLs


DB table
id, long_url, short_url, created_at, created_by

apis :
/getTinyUrl
/getLongUrl

Data is stored in DB and in a distributed cache

=> getTinyUrl
(1) check in distributed cache, if found then return
(2) check in DB , if found then return
(3) create tiny url for long url
(4) save in DB
(5) save in distributed cache


=> getLongUrl
(1) check in distributed cache, if found then return
(2) check in DB , if found then return


(Q) How to convert long string to short unique string
-> first we need to generate unique hash via MD5 or SHA256
-> then perform Base26 or Base64 encoding on that hash


(Q) what should be ttl for cache key
-> 1 day

(Q) how to delete expired urls
-> cron job should run after 24 hours and delete urls older than 5 days

(Q) how to track click stats
-> click stats should be maintained in distributed cache and
   should be updated in DB in separate table via cron
   cron time for stats update should be lesser (preferably half time) than delete cron


======================

Design facebook feed/Twitter page

facebook feed page - post
user -> friends
user -> post text/images
user -> can see post of his friends
post -> like, share, comment
//post will be visible on priority basis, based on time, num_of_comments and num_of_likes



User table
id, name, password, status

Friend table
id, user_id_1, user_id_2, status

Post table
id, post, posted_by, posted_at

Comment table
id, comment, post_id, commented_by, commented_at


apis
/createPost
/feedpage/{user_id}
/getFriends/{user_id}
/getComments/{post_id}


=> Approach 1

-> createPost
db insert post


-> feedpage , show paginated data (page_size=10, page_number=1)
get all friends (store friends in distributed cache, ttl = ? )
get top 10 latest post of each friend from DB
merge whole list into final 10 posts and return


=> Approach 2

Creation of new facebook post will save data in mysql
and push data in a pipeline (debizium --> kafka --> storm)
and every user related feeds are stored in a distributed cache like redis

Reads will be fulfilled from redis cache itself
No need to fire query on DB in case of Reads

kafka -->  Feed service --> Timeline service

-> Redis cache structure :
Every post or comment will be pushed to pipeline
Feed service will find all friends of the author of post
This post is pushed to all buckets (sorted set) of users in redis

sorted set logic by { time, num_of_likes, num_of_comments }
user1 --> [post_id_1, post_id_2]
user2 --> [post_id_1, post_id_2, post_id_3]


In this approach we have pre-populated all data of user feed in redis.
If any part of system is down, it will seen Eventually.
So it is Eventually consistent system.

(Q) If pipeline is down for a short time, how will it ensure that every packet is processed.
-> Using kafka with Storm will ensure at least once delivery
https://medium.com/@madhur25/meaning-of-at-least-once-at-most-once-and-exactly-once-delivery-10e477fafe16


(Q) If a user has 100k friends, then on his single post system will trigger 100k packets in redis.
This will be a bottleneck for redis, high load on timeline service.
-> For this special case, we can apply another logic of fetching data from DB (mixture of both DB and redis)
-> Or we can create separate redis keys for these special users, for these cases redis will store single copy of post.

=======================

Design dropbox or google drive

1) files can be uploaded or downloaded by users
2) files can be shared accross multiple devices and between multiple users
3) maintain history of changes in files


=> Approach 1

upload files to amazon S3, for scale, availabity and CDN properties
upload whole file to S3 when created
On any changes in file, upload full file to different path on S3 with different version
All these file meta informations can be stored in DB.

problems with this approach :
It consumes more internet bandwidth as on every update we upload full file again
It consumes more S3 space on higher updates (S3 space costs to us)

=> Approach 2

Divide the file into chunks of fixed size and upload these chunks in S3.
This will reduce latency and reduce S3 space on higher number of updates.
Chunk file uploaded to S3 will be having a unique random name based on some hash value.
These hash names, chunk number, file name, file type, file size
and all other similar information is stored in file_metadata.
This metadata is stored in DB.


Now how to upload file chunks to server.
To solve this problem we will use client side code (mobile client or web browser app)

Every client will be having a watcher service that watches on File changes.
Once a file is changed, the chunk file that is changed is uploaded to S3
and its metadata is created and updated to server via some api.
This metadata is saved to DB and cache.
Also this metadata is pushed to a queue like kafka , where it sends updated metadata packet to all other clients.
This way all clients sharing that file will be notified.
Advantage of queue is that, if client is offline for some time and it came again
then kafka can hold buffer data for a while.

mysql can be used to store chunk file metadata.

https://towardsdatascience.com/system-design-analysis-of-google-drive-ca3408f22ed3
https://www.youtube.com/watch?v=U0xTu6E2CT8&list=RDCMUCn1XnDWhsLS5URXTi5wtFTA&start_radio=1

============================

=> Uber system design :

- Web sockets that sends information of drivers
- Information is pushed to kafka and then saved in db and served by region servers
- Region servers have consistent  hashing for drivers location cells and gossip protocol for information of other servers
- Heavy write on db, db is geo based
- It uses google S2 library for cabs location and google map for eta between two locations
- Fault detection system, cross check driver location data
- Nodes can be used for websockets part

============================

=> Book my show :

- Central server that saves live movie ticket data
- While payment of seat booking , lock is acquired on the seat of movie by central server
- Central server must provide apis for checkout which internally applies lock, only unlocked and unsold seats are shown to user via apis
- As soon as, user goes to payment page,  seat and user data is saved in cache of BMS and central server is updated via push api. Cache can be distributed like redis on bus.

============================

=> cricinfo system :

- product apis and core apis
- 2 tier caching and 3 tier caching
- 2 tier : local cache and distributed cache, problem : stampede effect
- varnish : merges many requests to one request
- 2 and 3 tier caching must have some metadata information in response that shows which part resolved data. This would be helpful in debugging
- web sockets are used to push live scores to users

============================

Advantages and disadvantages of using micro services
https://www.cio.com/article/3201193/7-reasons-to-switch-to-microservices-and-5-reasons-you-might-not-succeed.html

advantages of using microservices :
(1) if one service has some problem, it does not impact the other service
(2) deployment part is segregated
(3) ownership of resources(database) is divided, any update is done via http call
(4) relatively less down time (high resilience)
(5) horizontal scaling of servers which results in less expense


disadvantage of using microservices :
(1) sometimes debugging of an issue causes multiple service scanning
(2) its difficult to make things transactional and complicated business scenarios
(3) end to end testing is complicated
(4) keep failures in mind
(5) high monitoring systems


==============================

=> Coupon code checkout system

Design a system for assigining promocodes to user while validating few dynamic conditions like
only 100 users can use a promocode
only Rs 1000 budget is allocated for a promocode

=>types of dynamic conditions :
total promocode usage cost
total promocode usage count
promocode usage per user
promocode usage per card
and many more like this


Problem : How to take a lock at promocode level
-> DB promocode table row level lock
-> Use distributed cache like redis and set key = promocode



Approach 1 :
checkout api
(1) take a lock at promocode level
(2) check if any dynamic condition fails from mysql, if fail then return
(3) If pass then, save order data in mysql
(4) release lock

Disadvantage of approach 1 :
(1) Heavy Read and Write operation on single mysql table
(2) For dynamic conditions, we will have aggregation queries (inner joins), so query will slow down on such big table
(3) As conditions may increase in future, system will become slower


Approach 2 : (store aggregated data in cassandra or some othe data source)
checkout api
(1) take a lock at promocode level
(2) check if any dynamic condition fails from cassandra, if fail then return
(3) If pass then, save order data in mysql
(4) save aggregated data in cassandra (sync call)
(5) release lock


Advantage of Approach 2 :
(1) Storing aggregated data in cassandra will avoid inner joins in mysql
	cassandra tables should be designed in such a way that supports our system queries
(2) Saving data in mysql can help us in auditing, machine learning, data analyses


Disadvantage of above api
(1) conditions can increase in future, so api may be more bulky
(2) different coditions require different locks, applying a single lock will downgrade api performance


Approach 3 : (saving data in cassandra will be async)
checkout api

-> Do an asyn call to cassandra
-> Take a condition based lock (one lock per condition) in a distributed cache just before saving data in mysql
-> When data is saved in cassandra condition table (different tables for checking different conditions)
	then release condition based lock


(1) check if any dynamic condition fails from cassandra, if fail then return
(2) If pass then, take condition based lock, i.e. save (user + promocode + condition) key in redis
(3) save order data in mysql

(1) In an async call , save aggregated data to cassandra (debizium, kafka-connect, storm consumer)
(2) When data is saved in cassandra successfully, release condition lock
	i.e. delete (user + promocode + condition) key from redis



Advantages of Approach 3 :
(1) lock is acquired on the basis of (user + promocode + condition),
	So two seperate promocodes or two seperate users can run parallely.
	This will be increase its performance

(2) saving data in cassandra is async call, so api responded very fast.


Problems of Approach 3 :
(1) What if data saved in mysql but some error occured while saving it in cassandra
(2) More tech stack is added in async update (debizium, kafka connect, storm), what if they are down
	These problems will make system in an inconsistent state
(3) What should ideal TTL value of (user + promocode + condition) key
	-> if async system is down and ttl is high, then other users will not be able to use promocodes

Solutions :
(1) lock will only be released when cassandra is updated successfuly.
	We can tweek the consistency level of cassandra writes for better performance.
(2) Async pipeline is dependent on kafka offset commit strategy and at least once approach.
(3) Also we have to make async packet processing as idempotent.
(4) If anything goes down, an ideal TTL for blocking user to avoid over use can be 30 minutes.

==========================================
