index creation 
-> static settings
	-> number of shards
	-> codec
	-> routing_partition_size
	-> soft_deletes.enabled
-> dynamic settings
	-> number_of_replicas
	-> search.idle.after
	-> refresh_interval
	-> max_result_window
	-> index.write.wait_for_active_shards

=================================

https://www.elastic.co/blog/found-elasticsearch-top-down


Cordinator Node :

The ES node which excepts request is called cordinator node.
Its job is route this request to proper node and proper shard.
For routing this request to proper node, every cordinator node must have whole cluster setting.
Every cordinator node must have information like
-> which nodes host which indexes and shards
-> metadata about every node
-> index mappings and templates


=>Replication and consistency :

Data is written down to primary shards. For resilency and HA multiple copies of data is stored in replica shards.
By default only primary shards are written (wait_for_active_shards=1)
max value of wait_for_active_shards = number_of_replicas+1

eg: If we have 3 nodes in cluster and index has number_of_replicas=3
then data is copied to 3 active shards : one primary shard and 2 replica shard
If number of active shards is less than 3, then index operation will not proceed successfuly.
This check is done when writes on few shards are done,
so if write is done primary shard and failed on replica shard, then _shard key will show these results.


=> What is an ES index

Conceptually, an Elasticsearch index with two shards is exactly the same as two Elasticsearch indexes with one shard each.
The difference is it provides routing feature in formar case.
Elasticsearch index is an abstraction on top of a collection of Lucene indexes, through the concept of shards.

=> Factors to consider for an index request
(1) routing key and routing config of shards
(2) write consistency value
(3) document associated unique id (try to assign IDs to make request idempotent)

=> Routing
For a good partitioning of data, routing key should be picked smartly.


=> Primary concerns
Every Shard has exactly one primary and zero or more replicas.
For every index operation primary shard will act as a cordinator
and will send the index operation to replicas depending upon consistency values.

When sufficient number of replicas acknowledge,
then primary shard will respond success or failure to cordinating node.

For a bulk operation, index request are sent to all respective primary shards in parallel.
ES has a transaction logs or translog (write ahead log)
Every request is written to translog and committed to index after some fix time.
When node crashes, data written to translog which is not commit are not reflected.


Lucene does not understand type of field or mapping of field.
Elastic Search does this job for us to make our task easier.
Lucene only knows to reverse indexed tokens.
(Analyser, filter, tokenizer)

Lucene do not have match query , ES have match query


=> Query then fetch (two rounds of searching)
lets suppose a query is fired to get top 10 records from whole index
-> query is fired on every shard to get top 10
-> these top 10 document ids will be merged by cordinator and final fetch is fired by Ids

====================================

https://www.elastic.co/blog/found-elasticsearch-top-down
https://www.elastic.co/blog/found-elasticsearch-from-the-bottom-up/
https://www.elastic.co/blog/found-sizing-elasticsearch
https://www.elastic.co/blog/found-optimizing-elasticsearch-searches/
https://www.elastic.co/blog/found-elasticsearch-in-production


https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html

https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html

===================================

==> ES issues and learnings :

index
5 shard  ---> 1 master and 2 replicas
each shard --> many segments

=============================

https://www.outcoldman.com/en/archive/2017/07/13/elasticsearch-explaining-merge-settings/

https://www.alibabacloud.com/blog/elasticsearch-distributed-consistency-principles-analysis-1---node_594358
https://www.alibabacloud.com/blog/elasticsearch-distributed-consistency-principles-analysis-2---meta_594359
https://alibaba-cloud.medium.com/elasticsearch-distributed-consistency-principles-analysis-3-data-a98cc436bc6b

===============================

properties :
-> max_merged_segment = 5gb default (merged segment should not be bigger than this size)
-> max_merge_at_once = 10 default  (how many segments can be merged at once)
-> max_segments_per_tier = 10 default (after how many segments creation, merge will be triggered)

1 tier has same size of segments, when they merge they move to next tier

merge algo logic :
-> first calculate which segments are eligible for merging
-> then calculate sum of segments which are less than max_merged_segment

solution :
tune merge ->
merge should be triggered less frequently
merge segments size should be less


max_merge_at_once is set to 5
max_segments_per_tier is set to 20

Ques => can we delete old data from ES which is not used currently

============================

==> Useful findings :

-> more replica will take more time in write query
-> If we increase number of shards in an index, then load on single shard will be low comparativily

-> data nodes should be equaly distributed accross all master and slave nodes.

for example :
if we have 15 shards, 1 primary and 2 replica each
total = 15 primary + 30 replica shards = 45
If we have 20 data nodes, then 15 nodes have 2 shards and 5 nodes have 3 shards


if we have 15 shards, 1 primary and 1 replica each
total = 15 primary + 15 replica shards = 30
If we have 20 data nodes, then 10 nodes have 2 shards and 10 nodes have 3 shards

In this way we have to equally distribute the load on data nodes.

Increasing shards of an index at runtime is not easy.
You have to make your index as read only and then create a copy of it to another large shard index.

============================
