Java api for kafka
https://static.javadoc.io/org.apache.kafka/kafka-clients/2.1.1/org/apache/kafka/clients/producer/KafkaProducer.html
https://static.javadoc.io/org.apache.kafka/kafka-clients/2.1.1/org/apache/kafka/clients/consumer/KafkaConsumer.html


Apache Kafka

https://www.youtube.com/watch?v=el-SqcZLZlI
https://www.youtube.com/watch?v=9RMOc0SwRro

production issues with kafka
** https://www.youtube.com/watch?v=1vLMuWsfMcA

https://www.youtube.com/watch?v=wMLAlJimPzk
https://www.youtube.com/watch?v=tJ1uIHQtoNc

kafka architecture blog
http://cloudurable.com/blog/kafka-architecture/index.html

zookeper features
https://data-flair.training/blogs/zookeeper-in-kafka/

=================

(Q) Diff bw kafka and rabbitmq

https://content.pivotal.io/blog/understanding-when-to-use-rabbitmq-or-apache-kafka

https://www.youtube.com/watch?v=lwMjjTT1Q-Q&list=PLSWDCWn6N_eF4mAt2O_VhTWUrj-kTQe48

https://content.pivotal.io/blog/understanding-when-to-use-rabbitmq-or-apache-kafka

==============================================
Apache kafka  : distributed streaming platform

Advantages : 
->enterprise messaging system
->stream processing
->connectors to import and export bulk data from database


enterprise messaging system
producers and consumers
P1		C1	
P2		C2
P3		C3

P1 -- C1, C3
P2 -- C2, C3
P3 -- C3
many more systems and many more combinations will be added as system grows
This problem was faced at linkedin and they came up with a solution of kafka

Producers --> Message Broker  --> Consumer

message Broker ==> kafka cluster with many brokers , zookeper

==============================================

Terminologies :

Producer
Consumer
Broker
Cluster
Topic
Partitions
Offset
Consumer group

=>Producer 	: Application that sends data/message record to kafka
=>Consumer 	: application that request broker to recieve data sent from producer
=>Broker 		: kafka Server that keeps data in the form of array of bytes
				message do not contain consumers address, 
				producer just send relevant message to the broker without any consumer information
				Consumer request broker to get data from a particular Producer

=>Topic 		: unique name for a kafka stream, it signifies type of data inside it
                    Topics in Kafka are always multi-subscriber; that is,
                    a topic can have zero, one, or many consumers that subscribe to the data written to it.

=>Cluster 	: running multiple instances of same application
=>Partition 	: Topic can be broken and stored on diff systems known as partitions
				It is like sharding of Topic, which is done by producers.
				topics in a partition are ordered but there is no global ordering of topic in different partitions.
				For global ordering of topics in different partitions, developer can add shard keys

=>Offset 		: messages in a broker are stored as an array in an order arrival time
				old messages to new messages (0 to N)
				this an array number which is immutable

Offsets are local to partition
There is no global offset
For locating a message we should have 3 things :
	Topic Name + Partition Number + Offset

=>Consumer Group : Acting as a single logical unit
				when multiple consumers share same kind of messages
When millions of data is pushed in broker, and single consumer has to handle this data
This single consumer can be divided into multiple consumers handling same kind of requests
In this example, broker can also be divided into many partitions 
and each consumer in consumer group is alligned with some set of partitions.

partitions P1, P2, P3, P4
Consumers C1, C2

C1 <---takes message from---P1, P2
C2 <---takes message from---P3, P4

Partitioning and Consumer group helps us to scale our system.

Note :
transferring data from partitions to consumer group has use cases as:
-> publisher subscriber
-> Message Queue

Every partition has a retention policy of holding a topic upto a limit of :
-> time , eg : 7 days
-> size of partition

==============================================

(Q) Why zookeper is required to use kafka ?

Zookeper is used for reliable distributed coordination

Naming
synchronization accross clusters
Messaging
Leader election
Notification

-> configuration information
-> naming
-> providing distributed synchronization
-> providing group services
-> centralized service 

zookeper server takes machine information and stores it in a System state log.
Master slave architechture is supported in zookeper,
where multiple servers send their log files data master server
Each client machine communicates with one of the zookeper servers to fetch machine data.

Kafka, Storm, HBase, SolrCloud uses zookeper
mongodb and elasticsearch created their own in house product for this.

==============================================

https://www.javainuse.com/spring/spring-boot-apache-kafka-hello-world

https://docs.spring.io/spring-kafka/reference/html/index.html


3 paradigms of programming :
-> request response
-> Batch
-> Stream processing => mix of above two

Async applications
large scale data
multiple processes are communicating with multiple other processes
Scalability and partitioning
fault tolerance

----------------------------------------------------------------

Installation : 

https://dzone.com/articles/kafka-setup

-----------------------------------------------------------------

// start zookeper
// default port 2181
bin/zookeeper-server-start.sh config/zookeeper.properties

// start kafka server

bin/kafka-server-start.sh config/server.properties


// topic creation 
// 2181 is zookeper port

bin/kafka-topics.sh --create 

bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test


// show topics
bin/kafka-topics.sh --list 

bin/kafka-topics.sh --list --zookeeper localhost:2181

// sending data from producer to a topic
// 9092 is kafka broker port

bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test


// start a consumer with a given topic

bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning

NOTE : broker stores data by producer for a given time.Once you read data from a consumer it is still managed by broker.
Now if you create new consumer and want to read data from same topic it will get full data.


-> bin/kafka-topics.sh 
	--describe
	--create
	--list
	--delete

-> bin/kafka-server-start.sh
-> bin/kafka-console-producer.sh
-> bin/kafka-console-consumer.sh



---------------------------------------------------

Kafka with java

https://dzone.com/articles/kafka-producer-and-consumer-example

Book : 
https://www.cloudera.com/documentation/enterprise/6/latest/PDF/cloudera-kafka.pdf

---------------------------------------------------

Record: Producer sends messages to Kafka in the form of records. 
A record is a key-value pair. It contains the topic name and partition number to be sent. 
Kafka broker keeps records inside topic partitions. 
Records sequence is maintained at the partition level. 
You can define the logic on which basis partition will be determined. 


Partition: A topic partition is a unit of parallelism in Kafka, i.e. 
two consumers cannot consume messages from the same partition at the same time.
 A consumer can consume from multiple partitions at the same time.

After a topic is created you can increase the partition count but it cannot be decreased

Consumer is subscribed to a topic


ENABLE_AUTO_COMMIT_CONFIG: When the consumer from a group receives a message it must commit the offset of that record. 
If this configuration is set to be true then, periodically, offsets will be committed, 
but, for the production level, this should be false and an offset should be committed manually.

Key Value pair in kafka :
Keys are used to determine the partition within a log to which a message get's appended to. 
While the value is the actual payload of the message.


** The Kafka cluster durably persists all published recordsâ€”whether or not they have been consumed
Kafka's performance is effectively constant with respect to data size so storing data for a long time is not a problem.


** Offset is controlled by the consumer.
A consumer can reset to an older offset to reprocess data from the past or
 skip ahead to the most recent record and start consuming from "now".
We can use kafka command line tools to "tail" the contents of any topic without changing 
what is consumed by any existing consumers.


** The producer is responsible for choosing which record to assign to which partition within the topic.


** Consumers label themselves with a consumer group name
Each record published to a topic is delivered to one consumer instance within each subscribing consumer group
Each group is composed of many consumer instances for scalability and fault tolerance
If all the consumer instances have the same consumer group, then the records will effectively be load balanced over the consumer instances.
If all the consumer instances have different consumer groups, then each record will be broadcast to all the consumer processes.



** Order of records is maintained within a partition
no order of records is maintained in multiple partitions
every partition is assigned a key
if you require a total order over records this can be achieved with a topic that has only one partition,
 though this will mean only one consumer process per consumer group.


===========================

Topic T has 3 partitions
P0
P1
P2


Consumer Group CG1, CG2

CG1 -> C1, C2
CG2 -> C3, C4


when a new consumer is created, it subscribe itself with a topic partition
P0 -> C1, C2   		// invalid
C1 <- P0, P1, P2	// valid


(4 partitions and 3 consumers in 1 consumer group)

producer	->	Topic		->	Consumer Group

producer	->	partition1	->	consumer1
			partition2	->	consumer2
			partition3	->	consumer3
			partition4	->	consumer1


each partition is consumed by exactly one consumer in the group
there cannot be more consumer instances in a consumer group than partitions.
===========================


Kafka allows producers to wait on acknowledgement so that a write isn't considered complete 
until it is fully replicated and guaranteed to persist even if the server written to fails.


Producer  Consumer
   |	     |
   |	     |
pushes     pulls
   |	     |
   |	     |
  Broker-Server


producer pushes data to broker
consumer pulls data from broker

A pull-based system has the nicer property that the consumer simply falls behind and catches up when it can
 it lends itself to aggressive batching of data sent to the consumer
kafka have parameters in their pull request that allow the consumer request to block in a "long poll" waiting until data arrives


broker will only store data, it will not be responsible for sending any data.


The leader keeps track of the set of "in sync" nodes. If a follower dies, gets stuck, or falls behind, 
the leader will remove it from the list of in sync replicas

Log compaction gives us a more granular retention mechanism so that we are guaranteed 
to retain at least the last update for each primary key  (e.g. bill@gmail.com).



==========================

=> start zookeper:

bin/zkServer.sh start

bin/zkServer.sh status

bin/zkServer.sh stop


=> check port 

netstat -tulpn | grep 2181


=============================


//@Configuration
public class CsKafkaConfig {

    @Value("${kafka.cs_answer_topic}")
    private String csAnswerTopic;

    //@Bean
    public NewTopic newTopic() {
        int partitions = 1;
        short replicationFactor = 1;
        return new NewTopic(csAnswerTopic, partitions, replicationFactor);
    }
}




//@Service
public class AnswerProducer {

    /*private static final Logger LOG = LogManager.getLogger(AnswerProducer.class);

    @Autowired
    private CsKafkaConfig csKafkaConfig;

    @Value("${kafka.cs_answer_topic}")
    private String csAnswerTopic;

    @Autowired
    private KafkaTemplate<String, AnswerDetail> kafkaTemplate;

    public void send(AnswerDetail message) throws Exception {
        LOG.info("sending message='{}' to topic='{}'", message, csAnswerTopic);
        kafkaTemplate.send(csAnswerTopic, message);
    }*/
}




//@Service
public class AnswerConsumer {

    /*private static final Logger logger = LogManager.getLogger(AnswerConsumer.class);

    @Autowired
    private KafkaTemplate<String, AnswerDetail> kafkaTemplate;

    @Autowired
    private AnswerService answerService;

    @KafkaListener(topics = "${kafka.cs_answer_topic}")
    public void processMessage(AnswerDetail answerDetail) {
        logger.info("answer payload received : {}", answerDetail.toString());
        try {
            answerService.saveAnswer(answerDetail);
        } catch (Exception e) {
            logger.error("Exception occured while saving answer :{}",answerDetail.toString(), e);
        }
    }*/
}




#kafka producer properties
kafka.cs_answer_topic=cs_answer
spring.kafka.producer.bootstrap-servers=localhost:9092
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer

#kafka consumer properties
spring.kafka.consumer.bootstrap-servers=localhost:9092
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializer
spring.kafka.consumer.group-id=cs_answer_grp


=============================


**The consumer maintains TCP connections to the necessary brokers to fetch data
The consumer is not thread-safe.

**Consumer is subscribed to a topic
A topic can have one or more than one partitions
Kafka will deliver each message in the subscribed topics to one process in each consumer group. 
This is achieved by balancing the partitions between all members in the consumer group 
so that each partition is assigned to exactly one consumer in the group. 
So if there is a topic with four partitions, and a consumer group with two processes, 
each process would consume from two partitions.

Membership in a consumer group is maintained dynamically: 
if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group. 
Similarly, if a new consumer joins the group, partitions will be moved from existing consumers to the new one. 
This is known as rebalancing the group
Group rebalancing is also used when new partitions are added to one of the subscribed topics 

It is also possible for the consumer to manually assign specific partitions
In this case, dynamic partition assignment and consumer group coordination will be disabled.


**As part of group management, the consumer will keep track of the list of consumers that belong
 to a particular group and will trigger a rebalance operation if any one of the following events are triggered:
->Number of partitions change for any of the subscribed topics
->A subscribed topic is created or deleted
->An existing member of the consumer group is shutdown or fails
->A new member is added to the consumer group

When any of these events are triggered, the provided listener will be invoked first to indicate that 
the consumer's assignment has been revoked, and then again when the new assignment has been received. 
Note that rebalances will only occur during an active call to poll(Duration), 
so callbacks will also only be invoked during that time.



Setting enable.auto.commit true means that offsets are committed automatically
 with a frequency controlled by the config auto.commit.interval.ms.

======================================

=>configuration of the producer


bootstrap.servers
key.serializer
value.serializer

acks -- [all, -1, 0, 1]
	The number of acknowledgments the producer requires the leader to have received before considering a request complete.

buffer.memory -- The total bytes of memory the producer can use to buffer records waiting to be sent to the server.

compression.type -- default is none


=> configuration for the consumer:


bootstrap.servers
key.deserializer
value.deserializer

fetch.min.bytes : The minimum amount of data the server should return for a fetch request.
	If insufficient data is available the request will wait for that much data to accumulate before answering the request

group.id : consumer group name

session.timeout.ms: The timeout used to detect client failures when using Kafka's group management facility.
	The client sends periodic heartbeats to indicate its liveness to the broker.
	If no heartbeats are received by the broker before the expiration of this session timeout,
	then the broker will remove this client from the group and initiate a rebalance.

enable.auto.commit:
	Default: true

partition.assignment.strategy
	class implementing ConsumerPartitionAssignor
	default : RangeAssignorValid

auto.offset.reset
	https://stackoverflow.com/questions/32390265/what-determines-kafka-consumer-offset

==========================================
https://kafka.apache.org/documentation/


Replicated Logs: Quorums, ISRs, and State Machines
Availability and Durability Guarantees
Log Compaction
Log Compaction Basics
4.9 Quotas

5.4 Log
5.5 Distribution

==========================================

Useful kafka commands


// push data to kafka topic
bin/kafka-console-producer.sh --broker-list <kafka_broker_ip>:9092 --topic <topic_name> --property "parse.key=true" --property "key.separator=::"
>your_kafka_packet_key::your_kafka_packet_value


// read data from kafka topic
bin/kafka-console-consumer.sh --bootstrap-server <kafka_broker_ip>:9092 --topic <topic_name>


// read data from kafka topic from beginning
bin/kafka-console-consumer.sh --bootstrap-server <kafka_broker_ip>:9092 --topic <topic_name> --from-beginning


// read kafka topic data at particular offset
bin/kafka-console-consumer.sh --bootstrap-server <kafka_broker_ip>:9092 --topic <topic_name> --max-messages 1  --partition <parition> --offset <offset>


// check offset value of all partitions in a consumer group (position of all consumers in a consumer group)
bin/kafka-consumer-groups.sh --bootstrap-server <kafka_broker_ip>:9092 --group <Consumer_group_name> --describe


// get all consumer groups across all topics
bin/kafka-consumer-groups.sh --bootstrap-server <kafka_broker_ip>:9092 --list


// dry run command for setting commited offset value in a partition
bin/kafka-consumer-groups.sh --bootstrap-server <kafka_broker_ip>:9092 --group <Consumer_group_name> --topic <topic_name>:<partition_number> --reset-offsets --to-offset <offset_number> --dry-run


// execute command for setting commited offset value in a partition
bin/kafka-consumer-groups.sh --bootstrap-server <kafka_broker_ip>:9092 --group <Consumer_group_name> --topic <topic_name>:<partition_number> --reset-offsets --to-offset <offset_number> --execute



// create kafka topic with partition=3, replica=2 and packet retention time=14 days
bin/kafka-topics.sh --create --zookeeper <zookeeper_ip>:2181 --replication-factor 2 --partitions 3 --topic <topic_name> --config retention.ms=1209600000


// list topic name
bin/kafka-topics.sh --list --zookeeper <zookeeper_ip>:2181


==========================================
